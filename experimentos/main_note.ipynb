{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPool2D, GlobalMaxPool2D\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the training set:\n",
    "file = open(r'data\\ucfTrainTestlist\\trainlist01.txt', 'r')\n",
    "temp = file.read()\n",
    "videos = temp.split('\\n')\n",
    "\n",
    "# Creating a dataframe:\n",
    "train = pd.DataFrame()\n",
    "train['video_name'] = videos\n",
    "\n",
    "# Dropping the last row:\n",
    "train = train[:-1]\n",
    "\n",
    "\n",
    "# Opening the test set:\n",
    "file_test = open(r'data\\ucfTrainTestlist\\testlist01.txt', 'r')\n",
    "temp = file_test.read()\n",
    "videos_test = temp.split(\"\\n\")\n",
    "\n",
    "test = pd.DataFrame()\n",
    "test['video_name'] = videos_test\n",
    "\n",
    "test = test[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding the video name:\n",
    "train['class'] = train['video_name'].str.split(\" \").apply(lambda x: x[1])\n",
    "\n",
    "train['class_name'] = train['video_name'].str.split(\"/\").apply(lambda x: x[0])\n",
    "\n",
    "train['video_name'] = train['video_name'].str.split(\"/\").apply(lambda x: x[1].split(\" \")[0])\n",
    "\n",
    "\n",
    "# Expanding test set:\n",
    "\n",
    "test['class_name'] = test['video_name'].str.split(\"/\").apply(lambda x: x[0])\n",
    "\n",
    "test['video_name'] = test['video_name'].str.split(\"/\").apply(lambda x: x[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9537/9537 [24:45<00:00,  6.42it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Storing traning videos:\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    count = 0\n",
    "    \n",
    "    video_name = train['video_name'][i]\n",
    "    video_class = train['class_name'][i]\n",
    "\n",
    "    path = \"data/Videos/{}/{}\".format(video_class, video_name)\n",
    "\n",
    "    video = cv2.VideoCapture(path)\n",
    "    frame_rate = video.get(7) # O identificador 5 é o FPS.\n",
    "   \n",
    "    while video.isOpened():\n",
    "\n",
    "        frameId = video.get(1) # Identificador 1 é o frame ID.\n",
    "        ret, frame = video.read()\n",
    "\n",
    "        if ret != True:\n",
    "            break\n",
    "    \n",
    "\n",
    "        if frameId % math.floor(frame_rate) == 0:\n",
    "            filename = 'data/ucfTrainTestlist/train_1/' + video_name + '_frame{}.jpg'.format(count)\n",
    "            cv2.imwrite(filename, frame)\n",
    "            count+=1\n",
    "\n",
    "    \n",
    "    video.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armazenando todas as imagens em uma variável\n",
    "\n",
    "imagens = os.listdir(\"data/ucfTrainTestlist/train_1\")\n",
    "train_labels  =[i.split(\"_\")[1] for i in imagens]\n",
    "\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "train_df['imagens'] = imagens\n",
    "train_df['labels'] = train_labels\n",
    "\n",
    "train_df.to_csv(\"data/train_new.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c01.avi_frame0.jpg</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c02.avi_frame0.jpg</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c03.avi_frame0.jpg</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c04.avi_frame0.jpg</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c05.avi_frame0.jpg</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9532</th>\n",
       "      <td>v_YoYo_g25_c01.avi_frame0.jpg</td>\n",
       "      <td>YoYo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9533</th>\n",
       "      <td>v_YoYo_g25_c02.avi_frame0.jpg</td>\n",
       "      <td>YoYo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9534</th>\n",
       "      <td>v_YoYo_g25_c03.avi_frame0.jpg</td>\n",
       "      <td>YoYo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9535</th>\n",
       "      <td>v_YoYo_g25_c04.avi_frame0.jpg</td>\n",
       "      <td>YoYo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9536</th>\n",
       "      <td>v_YoYo_g25_c05.avi_frame0.jpg</td>\n",
       "      <td>YoYo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9537 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      imagens          labels\n",
       "0     v_ApplyEyeMakeup_g08_c01.avi_frame0.jpg  ApplyEyeMakeup\n",
       "1     v_ApplyEyeMakeup_g08_c02.avi_frame0.jpg  ApplyEyeMakeup\n",
       "2     v_ApplyEyeMakeup_g08_c03.avi_frame0.jpg  ApplyEyeMakeup\n",
       "3     v_ApplyEyeMakeup_g08_c04.avi_frame0.jpg  ApplyEyeMakeup\n",
       "4     v_ApplyEyeMakeup_g08_c05.avi_frame0.jpg  ApplyEyeMakeup\n",
       "...                                       ...             ...\n",
       "9532            v_YoYo_g25_c01.avi_frame0.jpg            YoYo\n",
       "9533            v_YoYo_g25_c02.avi_frame0.jpg            YoYo\n",
       "9534            v_YoYo_g25_c03.avi_frame0.jpg            YoYo\n",
       "9535            v_YoYo_g25_c04.avi_frame0.jpg            YoYo\n",
       "9536            v_YoYo_g25_c05.avi_frame0.jpg            YoYo\n",
       "\n",
       "[9537 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"data/train_new.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9537/9537 [01:00<00:00, 158.13it/s]\n"
     ]
    }
   ],
   "source": [
    "train_image = []\n",
    "\n",
    "for i in tqdm(range(train_df.shape[0])):\n",
    "\n",
    "    path = \"data/ucfTrainTestlist/train_1/\" + train_df[\"imagens\"][i]\n",
    "    img = image.load_img(path, target_size=(224, 224, 3))\n",
    "\n",
    "    img = image.img_to_array(img)\n",
    "\n",
    "    # Normalizando os pixels das imagens:\n",
    "    img = img/255\n",
    "\n",
    "    train_image.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9537, 224, 224, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df[\"labels\"]\n",
    "\n",
    "# Splitting data:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando os labels em variáveis dummy:\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model'S Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 8s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Inicializando o modelo pre-treinado:\n",
    "base_model = VGG16(weights='imagenet', include_top=False) # inlcude_top define se irá manter a ultima camada ou não."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congelando as camadas de treinamento\n",
    "#for layer in base_model.layers:\n",
    "#    layer.trainable = False\n",
    "\n",
    "#classifier1 = base_model.output#head mode\n",
    "#classifier1 = Flatten()(classifier1)#adding layer of flatten\n",
    "#classifier1 = Dense(units=256, activation='relu')(classifier1)\n",
    "#classifier1 = Dropout(0.6)(classifier1)\n",
    "#classifier1 = Dense(units=40, activation='softmax')(classifier1)\n",
    "\n",
    "#model = Model(inputs = base_model.input , outputs = classifier1)\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7629, 224, 224, 3)\n",
      "Test shape: (1908, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - 964s 4s/step\n",
      "60/60 [==============================] - 219s 4s/step\n",
      "Train shape: (7629, 7, 7, 512)\n",
      "Test shape: (1908, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "X_train = base_model.predict(X_train)\n",
    "X_test = base_model.predict(X_test)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the training as well as validation frames in single dimension\n",
    "X_train = X_train.reshape(7629, 7*7*512)\n",
    "X_test = X_test.reshape(1908, 7*7*512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_layer():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, activation='relu', input_shape=(7629, )))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(101, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fully_connected_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "mcp_save = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 4.8453 - accuracy: 0.0102"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caios\\Documents\\DataScience\\ComputerVision\\projects\\ActionRecognition\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 28s 429ms/step - loss: 4.8453 - accuracy: 0.0102 - val_loss: 4.6119 - val_accuracy: 0.0105\n",
      "Epoch 2/200\n",
      "60/60 [==============================] - 25s 424ms/step - loss: 4.6128 - accuracy: 0.0115 - val_loss: 4.6034 - val_accuracy: 0.0362\n",
      "Epoch 3/200\n",
      "60/60 [==============================] - 26s 439ms/step - loss: 4.5830 - accuracy: 0.0198 - val_loss: 4.5336 - val_accuracy: 0.0367\n",
      "Epoch 4/200\n",
      "60/60 [==============================] - 25s 412ms/step - loss: 4.5350 - accuracy: 0.0287 - val_loss: 4.4908 - val_accuracy: 0.0414\n",
      "Epoch 5/200\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 4.4826 - accuracy: 0.0317 - val_loss: 4.3924 - val_accuracy: 0.0655\n",
      "Epoch 6/200\n",
      "60/60 [==============================] - 24s 404ms/step - loss: 4.4314 - accuracy: 0.0388 - val_loss: 4.3301 - val_accuracy: 0.0613\n",
      "Epoch 7/200\n",
      "60/60 [==============================] - 24s 406ms/step - loss: 4.3650 - accuracy: 0.0380 - val_loss: 4.2367 - val_accuracy: 0.0729\n",
      "Epoch 8/200\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 4.3328 - accuracy: 0.0402 - val_loss: 4.2629 - val_accuracy: 0.0891\n",
      "Epoch 9/200\n",
      "60/60 [==============================] - 24s 408ms/step - loss: 4.2763 - accuracy: 0.0397 - val_loss: 4.0805 - val_accuracy: 0.0886\n",
      "Epoch 10/200\n",
      "60/60 [==============================] - 24s 399ms/step - loss: 4.2486 - accuracy: 0.0484 - val_loss: 4.1180 - val_accuracy: 0.0954\n",
      "Epoch 11/200\n",
      "60/60 [==============================] - 25s 412ms/step - loss: 4.2062 - accuracy: 0.0444 - val_loss: 4.0241 - val_accuracy: 0.1017\n",
      "Epoch 12/200\n",
      "60/60 [==============================] - 25s 413ms/step - loss: 4.1653 - accuracy: 0.0518 - val_loss: 3.9994 - val_accuracy: 0.1080\n",
      "Epoch 13/200\n",
      "60/60 [==============================] - 25s 417ms/step - loss: 4.1107 - accuracy: 0.0526 - val_loss: 3.9165 - val_accuracy: 0.1221\n",
      "Epoch 14/200\n",
      "60/60 [==============================] - 24s 405ms/step - loss: 4.1182 - accuracy: 0.0527 - val_loss: 3.9329 - val_accuracy: 0.1106\n",
      "Epoch 15/200\n",
      "60/60 [==============================] - 25s 423ms/step - loss: 4.0760 - accuracy: 0.0495 - val_loss: 3.9047 - val_accuracy: 0.1295\n",
      "Epoch 16/200\n",
      "60/60 [==============================] - 25s 416ms/step - loss: 4.0863 - accuracy: 0.0539 - val_loss: 3.8738 - val_accuracy: 0.1101\n",
      "Epoch 17/200\n",
      "60/60 [==============================] - 24s 403ms/step - loss: 4.0349 - accuracy: 0.0575 - val_loss: 3.8572 - val_accuracy: 0.1410\n",
      "Epoch 18/200\n",
      "60/60 [==============================] - 24s 402ms/step - loss: 3.9931 - accuracy: 0.0547 - val_loss: 3.7676 - val_accuracy: 0.1232\n",
      "Epoch 19/200\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 3.9697 - accuracy: 0.0575 - val_loss: 3.7322 - val_accuracy: 0.1415\n",
      "Epoch 20/200\n",
      "60/60 [==============================] - 23s 381ms/step - loss: 3.9397 - accuracy: 0.0633 - val_loss: 3.7733 - val_accuracy: 0.1373\n",
      "Epoch 21/200\n",
      "60/60 [==============================] - 24s 401ms/step - loss: 3.9191 - accuracy: 0.0620 - val_loss: 3.7269 - val_accuracy: 0.1572\n",
      "Epoch 22/200\n",
      "60/60 [==============================] - 23s 376ms/step - loss: 3.9193 - accuracy: 0.0613 - val_loss: 3.7284 - val_accuracy: 0.1137\n",
      "Epoch 23/200\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 3.8806 - accuracy: 0.0642 - val_loss: 3.6328 - val_accuracy: 0.1431\n",
      "Epoch 24/200\n",
      "60/60 [==============================] - 23s 376ms/step - loss: 3.8598 - accuracy: 0.0672 - val_loss: 3.6731 - val_accuracy: 0.1588\n",
      "Epoch 25/200\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 3.8427 - accuracy: 0.0700 - val_loss: 3.6001 - val_accuracy: 0.1751\n",
      "Epoch 26/200\n",
      "60/60 [==============================] - 23s 380ms/step - loss: 3.8396 - accuracy: 0.0680 - val_loss: 3.7086 - val_accuracy: 0.1452\n",
      "Epoch 27/200\n",
      "60/60 [==============================] - 23s 379ms/step - loss: 3.8154 - accuracy: 0.0734 - val_loss: 3.6191 - val_accuracy: 0.1572\n",
      "Epoch 28/200\n",
      "60/60 [==============================] - 24s 397ms/step - loss: 3.8005 - accuracy: 0.0729 - val_loss: 3.5948 - val_accuracy: 0.1520\n",
      "Epoch 29/200\n",
      "60/60 [==============================] - 23s 377ms/step - loss: 3.8089 - accuracy: 0.0751 - val_loss: 3.6320 - val_accuracy: 0.1441\n",
      "Epoch 30/200\n",
      "60/60 [==============================] - 24s 393ms/step - loss: 3.7745 - accuracy: 0.0755 - val_loss: 3.5518 - val_accuracy: 0.1667\n",
      "Epoch 31/200\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 3.7653 - accuracy: 0.0754 - val_loss: 3.5385 - val_accuracy: 0.1499\n",
      "Epoch 32/200\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 3.7444 - accuracy: 0.0809 - val_loss: 3.4853 - val_accuracy: 0.1824\n",
      "Epoch 33/200\n",
      "60/60 [==============================] - 23s 379ms/step - loss: 3.7545 - accuracy: 0.0762 - val_loss: 3.4940 - val_accuracy: 0.1792\n",
      "Epoch 34/200\n",
      "60/60 [==============================] - 23s 379ms/step - loss: 3.7092 - accuracy: 0.0840 - val_loss: 3.5366 - val_accuracy: 0.1525\n",
      "Epoch 35/200\n",
      "60/60 [==============================] - 23s 381ms/step - loss: 3.7616 - accuracy: 0.0771 - val_loss: 3.5540 - val_accuracy: 0.1530\n",
      "Epoch 36/200\n",
      "60/60 [==============================] - 23s 380ms/step - loss: 3.7194 - accuracy: 0.0861 - val_loss: 3.5024 - val_accuracy: 0.1651\n",
      "Epoch 37/200\n",
      "60/60 [==============================] - 23s 379ms/step - loss: 3.6764 - accuracy: 0.0856 - val_loss: 3.5011 - val_accuracy: 0.1614\n",
      "Epoch 38/200\n",
      "60/60 [==============================] - 23s 392ms/step - loss: 3.6821 - accuracy: 0.0869 - val_loss: 3.4685 - val_accuracy: 0.1635\n",
      "Epoch 39/200\n",
      "60/60 [==============================] - 23s 380ms/step - loss: 3.6994 - accuracy: 0.0893 - val_loss: 3.4952 - val_accuracy: 0.1635\n",
      "Epoch 40/200\n",
      "60/60 [==============================] - 23s 379ms/step - loss: 3.6525 - accuracy: 0.0914 - val_loss: 3.4883 - val_accuracy: 0.1478\n",
      "Epoch 41/200\n",
      "60/60 [==============================] - 23s 377ms/step - loss: 3.6999 - accuracy: 0.0903 - val_loss: 3.4966 - val_accuracy: 0.1452\n",
      "Epoch 42/200\n",
      "60/60 [==============================] - 24s 396ms/step - loss: 3.6570 - accuracy: 0.0883 - val_loss: 3.4475 - val_accuracy: 0.1630\n",
      "Epoch 43/200\n",
      "60/60 [==============================] - 23s 379ms/step - loss: 3.6511 - accuracy: 0.0915 - val_loss: 3.4722 - val_accuracy: 0.1520\n",
      "Epoch 44/200\n",
      "60/60 [==============================] - 23s 380ms/step - loss: 3.6348 - accuracy: 0.0895 - val_loss: 3.4775 - val_accuracy: 0.1572\n",
      "Epoch 45/200\n",
      "60/60 [==============================] - 23s 377ms/step - loss: 3.6690 - accuracy: 0.0907 - val_loss: 3.4560 - val_accuracy: 0.1599\n",
      "Epoch 46/200\n",
      "60/60 [==============================] - 23s 378ms/step - loss: 3.6635 - accuracy: 0.0898 - val_loss: 3.5127 - val_accuracy: 0.1368\n",
      "Epoch 47/200\n",
      "60/60 [==============================] - 25s 419ms/step - loss: 3.6285 - accuracy: 0.0984 - val_loss: 3.5193 - val_accuracy: 0.1509\n",
      "Epoch 48/200\n",
      "60/60 [==============================] - 25s 414ms/step - loss: 3.6149 - accuracy: 0.1008 - val_loss: 3.5559 - val_accuracy: 0.1211\n",
      "Epoch 49/200\n",
      "60/60 [==============================] - 25s 416ms/step - loss: 3.6276 - accuracy: 0.0966 - val_loss: 3.5304 - val_accuracy: 0.1316\n",
      "Epoch 50/200\n",
      "60/60 [==============================] - 25s 417ms/step - loss: 3.6233 - accuracy: 0.0961 - val_loss: 3.5203 - val_accuracy: 0.1357\n",
      "Epoch 51/200\n",
      "60/60 [==============================] - 27s 443ms/step - loss: 3.6179 - accuracy: 0.0983 - val_loss: 3.6015 - val_accuracy: 0.1006\n",
      "Epoch 52/200\n",
      "60/60 [==============================] - 25s 416ms/step - loss: 3.5886 - accuracy: 0.1062 - val_loss: 3.5770 - val_accuracy: 0.1137\n",
      "Epoch 53/200\n",
      "60/60 [==============================] - 28s 471ms/step - loss: 3.6022 - accuracy: 0.1037 - val_loss: 3.4888 - val_accuracy: 0.1462\n",
      "Epoch 54/200\n",
      "60/60 [==============================] - 31s 521ms/step - loss: 3.6015 - accuracy: 0.1016 - val_loss: 3.4468 - val_accuracy: 0.1462\n",
      "Epoch 55/200\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 3.5718 - accuracy: 0.1005 - val_loss: 3.4734 - val_accuracy: 0.1394\n",
      "Epoch 56/200\n",
      "60/60 [==============================] - 24s 398ms/step - loss: 3.5623 - accuracy: 0.0986 - val_loss: 3.5658 - val_accuracy: 0.1101\n",
      "Epoch 57/200\n",
      "60/60 [==============================] - 25s 417ms/step - loss: 3.6340 - accuracy: 0.0995 - val_loss: 3.5748 - val_accuracy: 0.1200\n",
      "Epoch 58/200\n",
      "60/60 [==============================] - 27s 450ms/step - loss: 3.6699 - accuracy: 0.0984 - val_loss: 3.6738 - val_accuracy: 0.0938\n",
      "Epoch 59/200\n",
      "60/60 [==============================] - 26s 425ms/step - loss: 3.5833 - accuracy: 0.1025 - val_loss: 3.6227 - val_accuracy: 0.0959\n",
      "Epoch 60/200\n",
      "60/60 [==============================] - 26s 433ms/step - loss: 3.5601 - accuracy: 0.1046 - val_loss: 3.5366 - val_accuracy: 0.1247\n",
      "Epoch 61/200\n",
      "60/60 [==============================] - 25s 422ms/step - loss: 3.5597 - accuracy: 0.1067 - val_loss: 3.5466 - val_accuracy: 0.1200\n",
      "Epoch 62/200\n",
      "60/60 [==============================] - 25s 421ms/step - loss: 3.5327 - accuracy: 0.1071 - val_loss: 3.4555 - val_accuracy: 0.1536\n",
      "Epoch 63/200\n",
      "60/60 [==============================] - 25s 420ms/step - loss: 3.5308 - accuracy: 0.1091 - val_loss: 3.5479 - val_accuracy: 0.1122\n",
      "Epoch 64/200\n",
      "60/60 [==============================] - 25s 420ms/step - loss: 3.5218 - accuracy: 0.1110 - val_loss: 3.5572 - val_accuracy: 0.1080\n",
      "Epoch 65/200\n",
      "60/60 [==============================] - 25s 420ms/step - loss: 3.5106 - accuracy: 0.1095 - val_loss: 3.4721 - val_accuracy: 0.1195\n",
      "Epoch 66/200\n",
      "60/60 [==============================] - 31s 524ms/step - loss: 3.4919 - accuracy: 0.1142 - val_loss: 3.5530 - val_accuracy: 0.1143\n",
      "Epoch 67/200\n",
      "60/60 [==============================] - 33s 557ms/step - loss: 3.4992 - accuracy: 0.1139 - val_loss: 3.5410 - val_accuracy: 0.1289\n",
      "Epoch 68/200\n",
      "60/60 [==============================] - 30s 493ms/step - loss: 3.4860 - accuracy: 0.1109 - val_loss: 3.5455 - val_accuracy: 0.1148\n",
      "Epoch 69/200\n",
      "60/60 [==============================] - 30s 490ms/step - loss: 3.5085 - accuracy: 0.1122 - val_loss: 3.5804 - val_accuracy: 0.1179\n",
      "Epoch 70/200\n",
      "60/60 [==============================] - 30s 497ms/step - loss: 3.5835 - accuracy: 0.1034 - val_loss: 3.6215 - val_accuracy: 0.1053\n",
      "Epoch 71/200\n",
      "60/60 [==============================] - 28s 468ms/step - loss: 3.6525 - accuracy: 0.0962 - val_loss: 3.7650 - val_accuracy: 0.0702\n",
      "Epoch 72/200\n",
      "60/60 [==============================] - 30s 507ms/step - loss: 3.6422 - accuracy: 0.1036 - val_loss: 3.5880 - val_accuracy: 0.1111\n",
      "Epoch 73/200\n",
      "60/60 [==============================] - 31s 510ms/step - loss: 3.6106 - accuracy: 0.1022 - val_loss: 3.6871 - val_accuracy: 0.0922\n",
      "Epoch 74/200\n",
      "60/60 [==============================] - 26s 427ms/step - loss: 3.6399 - accuracy: 0.1004 - val_loss: 3.6646 - val_accuracy: 0.0922\n",
      "Epoch 75/200\n",
      "60/60 [==============================] - 26s 426ms/step - loss: 3.5990 - accuracy: 0.1003 - val_loss: 3.5629 - val_accuracy: 0.1053\n",
      "Epoch 76/200\n",
      "60/60 [==============================] - 25s 423ms/step - loss: 3.5173 - accuracy: 0.1165 - val_loss: 3.5969 - val_accuracy: 0.1001\n",
      "Epoch 77/200\n",
      "60/60 [==============================] - 26s 426ms/step - loss: 3.5808 - accuracy: 0.1034 - val_loss: 3.7164 - val_accuracy: 0.0891\n",
      "Epoch 78/200\n",
      "60/60 [==============================] - 25s 422ms/step - loss: 3.4799 - accuracy: 0.1181 - val_loss: 3.5713 - val_accuracy: 0.1064\n",
      "Epoch 79/200\n",
      "60/60 [==============================] - 25s 425ms/step - loss: 3.4822 - accuracy: 0.1163 - val_loss: 3.6612 - val_accuracy: 0.0823\n",
      "Epoch 80/200\n",
      "60/60 [==============================] - 26s 424ms/step - loss: 3.4942 - accuracy: 0.1138 - val_loss: 3.7006 - val_accuracy: 0.1001\n",
      "Epoch 81/200\n",
      "60/60 [==============================] - 25s 423ms/step - loss: 3.5173 - accuracy: 0.1146 - val_loss: 3.6268 - val_accuracy: 0.1137\n",
      "Epoch 82/200\n",
      "60/60 [==============================] - 26s 434ms/step - loss: 3.5002 - accuracy: 0.1092 - val_loss: 3.6941 - val_accuracy: 0.0928\n",
      "Epoch 83/200\n",
      "60/60 [==============================] - 26s 429ms/step - loss: 3.5057 - accuracy: 0.1143 - val_loss: 3.5448 - val_accuracy: 0.1205\n",
      "Epoch 84/200\n",
      "60/60 [==============================] - 26s 433ms/step - loss: 3.4551 - accuracy: 0.1178 - val_loss: 3.6453 - val_accuracy: 0.1032\n",
      "Epoch 85/200\n",
      "60/60 [==============================] - 25s 415ms/step - loss: 3.4382 - accuracy: 0.1185 - val_loss: 3.7722 - val_accuracy: 0.0901\n",
      "Epoch 86/200\n",
      "60/60 [==============================] - 23s 381ms/step - loss: 3.4988 - accuracy: 0.1181 - val_loss: 3.6450 - val_accuracy: 0.0980\n",
      "Epoch 87/200\n",
      "60/60 [==============================] - 23s 382ms/step - loss: 3.4903 - accuracy: 0.1173 - val_loss: 3.6128 - val_accuracy: 0.1001\n",
      "Epoch 88/200\n",
      "60/60 [==============================] - 23s 380ms/step - loss: 3.4753 - accuracy: 0.1188 - val_loss: 3.8072 - val_accuracy: 0.0807\n",
      "Epoch 89/200\n",
      "60/60 [==============================] - 23s 382ms/step - loss: 3.4385 - accuracy: 0.1232 - val_loss: 3.7050 - val_accuracy: 0.0980\n",
      "Epoch 90/200\n",
      "60/60 [==============================] - 25s 424ms/step - loss: 3.4263 - accuracy: 0.1278 - val_loss: 3.6151 - val_accuracy: 0.1111\n",
      "Epoch 91/200\n",
      "60/60 [==============================] - 25s 423ms/step - loss: 3.3993 - accuracy: 0.1261 - val_loss: 3.6740 - val_accuracy: 0.1190\n",
      "Epoch 92/200\n",
      "60/60 [==============================] - 25s 424ms/step - loss: 3.5314 - accuracy: 0.1115 - val_loss: 3.6826 - val_accuracy: 0.0896\n",
      "Epoch 93/200\n",
      "60/60 [==============================] - 26s 425ms/step - loss: 3.4175 - accuracy: 0.1273 - val_loss: 3.6358 - val_accuracy: 0.1022\n",
      "Epoch 94/200\n",
      "60/60 [==============================] - 25s 424ms/step - loss: 3.4266 - accuracy: 0.1275 - val_loss: 3.6425 - val_accuracy: 0.0996\n",
      "Epoch 95/200\n",
      "60/60 [==============================] - 25s 424ms/step - loss: 3.3838 - accuracy: 0.1319 - val_loss: 3.6690 - val_accuracy: 0.0980\n",
      "Epoch 96/200\n",
      "60/60 [==============================] - 26s 429ms/step - loss: 3.4170 - accuracy: 0.1285 - val_loss: 3.6683 - val_accuracy: 0.0959\n",
      "Epoch 97/200\n",
      "60/60 [==============================] - 26s 428ms/step - loss: 3.3806 - accuracy: 0.1313 - val_loss: 3.7417 - val_accuracy: 0.0870\n",
      "Epoch 98/200\n",
      "60/60 [==============================] - 25s 423ms/step - loss: 3.4366 - accuracy: 0.1281 - val_loss: 3.7350 - val_accuracy: 0.1017\n",
      "Epoch 99/200\n",
      "60/60 [==============================] - 26s 435ms/step - loss: 3.4049 - accuracy: 0.1303 - val_loss: 3.5378 - val_accuracy: 0.1263\n",
      "Epoch 100/200\n",
      "60/60 [==============================] - 25s 424ms/step - loss: 3.4170 - accuracy: 0.1261 - val_loss: 3.7814 - val_accuracy: 0.0697\n",
      "Epoch 101/200\n",
      "60/60 [==============================] - 26s 426ms/step - loss: 3.3918 - accuracy: 0.1316 - val_loss: 3.6530 - val_accuracy: 0.1074\n",
      "Epoch 102/200\n",
      "60/60 [==============================] - 26s 426ms/step - loss: 3.3990 - accuracy: 0.1258 - val_loss: 3.6822 - val_accuracy: 0.1053\n",
      "Epoch 103/200\n",
      "60/60 [==============================] - 26s 426ms/step - loss: 3.3935 - accuracy: 0.1370 - val_loss: 3.7425 - val_accuracy: 0.0922\n",
      "Epoch 104/200\n",
      "60/60 [==============================] - 26s 429ms/step - loss: 3.3442 - accuracy: 0.1469 - val_loss: 3.6974 - val_accuracy: 0.1027\n",
      "Epoch 105/200\n",
      "60/60 [==============================] - 25s 424ms/step - loss: 3.4438 - accuracy: 0.1261 - val_loss: 3.7438 - val_accuracy: 0.1090\n",
      "Epoch 106/200\n",
      "60/60 [==============================] - 25s 425ms/step - loss: 3.4484 - accuracy: 0.1237 - val_loss: 3.7205 - val_accuracy: 0.1048\n",
      "Epoch 107/200\n",
      "60/60 [==============================] - 26s 428ms/step - loss: 3.3990 - accuracy: 0.1354 - val_loss: 3.8689 - val_accuracy: 0.0776\n",
      "Epoch 108/200\n",
      "60/60 [==============================] - 25s 421ms/step - loss: 3.3723 - accuracy: 0.1355 - val_loss: 3.7565 - val_accuracy: 0.0907\n",
      "Epoch 109/200\n",
      "60/60 [==============================] - 26s 427ms/step - loss: 3.3379 - accuracy: 0.1401 - val_loss: 3.7066 - val_accuracy: 0.1059\n",
      "Epoch 110/200\n",
      "60/60 [==============================] - 26s 425ms/step - loss: 3.3917 - accuracy: 0.1304 - val_loss: 3.7099 - val_accuracy: 0.0912\n",
      "Epoch 111/200\n",
      "60/60 [==============================] - 28s 472ms/step - loss: 3.3696 - accuracy: 0.1365 - val_loss: 3.8476 - val_accuracy: 0.0776\n",
      "Epoch 112/200\n",
      "60/60 [==============================] - 30s 504ms/step - loss: 3.3371 - accuracy: 0.1420 - val_loss: 3.8223 - val_accuracy: 0.0881\n",
      "Epoch 113/200\n",
      "60/60 [==============================] - 30s 496ms/step - loss: 3.3496 - accuracy: 0.1357 - val_loss: 3.7713 - val_accuracy: 0.0917\n",
      "Epoch 114/200\n",
      "60/60 [==============================] - 28s 465ms/step - loss: 3.3361 - accuracy: 0.1434 - val_loss: 3.8625 - val_accuracy: 0.0970\n",
      "Epoch 115/200\n",
      "60/60 [==============================] - 25s 424ms/step - loss: 3.7084 - accuracy: 0.1000 - val_loss: 4.1315 - val_accuracy: 0.0503\n",
      "Epoch 116/200\n",
      "60/60 [==============================] - 26s 427ms/step - loss: 3.7737 - accuracy: 0.0991 - val_loss: 4.0338 - val_accuracy: 0.0529\n",
      "Epoch 117/200\n",
      "60/60 [==============================] - 31s 523ms/step - loss: 3.6822 - accuracy: 0.1007 - val_loss: 3.7639 - val_accuracy: 0.0791\n",
      "Epoch 118/200\n",
      "60/60 [==============================] - 36s 596ms/step - loss: 3.4990 - accuracy: 0.1262 - val_loss: 3.8142 - val_accuracy: 0.0818\n",
      "Epoch 119/200\n",
      "60/60 [==============================] - 28s 463ms/step - loss: 3.3742 - accuracy: 0.1400 - val_loss: 3.8217 - val_accuracy: 0.0749\n",
      "Epoch 120/200\n",
      "60/60 [==============================] - 26s 425ms/step - loss: 3.3416 - accuracy: 0.1429 - val_loss: 3.6878 - val_accuracy: 0.1080\n",
      "Epoch 121/200\n",
      "60/60 [==============================] - 26s 438ms/step - loss: 3.3742 - accuracy: 0.1401 - val_loss: 3.8369 - val_accuracy: 0.0770\n",
      "Epoch 122/200\n",
      "60/60 [==============================] - 26s 432ms/step - loss: 3.3439 - accuracy: 0.1433 - val_loss: 3.7248 - val_accuracy: 0.0865\n",
      "Epoch 123/200\n",
      "60/60 [==============================] - 26s 435ms/step - loss: 3.3281 - accuracy: 0.1479 - val_loss: 3.8899 - val_accuracy: 0.0744\n",
      "Epoch 124/200\n",
      "60/60 [==============================] - 26s 431ms/step - loss: 3.3615 - accuracy: 0.1406 - val_loss: 4.1011 - val_accuracy: 0.0755\n",
      "Epoch 125/200\n",
      "60/60 [==============================] - 26s 433ms/step - loss: 3.4944 - accuracy: 0.1268 - val_loss: 3.8950 - val_accuracy: 0.0755\n",
      "Epoch 126/200\n",
      "60/60 [==============================] - 26s 431ms/step - loss: 3.4020 - accuracy: 0.1389 - val_loss: 3.8235 - val_accuracy: 0.0744\n",
      "Epoch 127/200\n",
      "60/60 [==============================] - 26s 433ms/step - loss: 3.3284 - accuracy: 0.1454 - val_loss: 3.9337 - val_accuracy: 0.0650\n",
      "Epoch 128/200\n",
      "60/60 [==============================] - 26s 436ms/step - loss: 3.3032 - accuracy: 0.1515 - val_loss: 3.8637 - val_accuracy: 0.0854\n",
      "Epoch 129/200\n",
      "60/60 [==============================] - 27s 448ms/step - loss: 3.3374 - accuracy: 0.1435 - val_loss: 3.7641 - val_accuracy: 0.0959\n",
      "Epoch 130/200\n",
      "60/60 [==============================] - 26s 439ms/step - loss: 3.3282 - accuracy: 0.1426 - val_loss: 3.9117 - val_accuracy: 0.0802\n",
      "Epoch 131/200\n",
      "60/60 [==============================] - 26s 433ms/step - loss: 3.3189 - accuracy: 0.1429 - val_loss: 3.8688 - val_accuracy: 0.0791\n",
      "Epoch 132/200\n",
      "60/60 [==============================] - 26s 434ms/step - loss: 3.4225 - accuracy: 0.1329 - val_loss: 3.8665 - val_accuracy: 0.0577\n",
      "Epoch 133/200\n",
      "60/60 [==============================] - 26s 426ms/step - loss: 3.3482 - accuracy: 0.1427 - val_loss: 3.8459 - val_accuracy: 0.0713\n",
      "Epoch 134/200\n",
      "60/60 [==============================] - 27s 451ms/step - loss: 3.2793 - accuracy: 0.1489 - val_loss: 3.8181 - val_accuracy: 0.0891\n",
      "Epoch 135/200\n",
      "60/60 [==============================] - 29s 477ms/step - loss: 3.3345 - accuracy: 0.1371 - val_loss: 3.8810 - val_accuracy: 0.0791\n",
      "Epoch 136/200\n",
      "60/60 [==============================] - 28s 473ms/step - loss: 3.3231 - accuracy: 0.1452 - val_loss: 3.8826 - val_accuracy: 0.0713\n",
      "Epoch 137/200\n",
      "37/60 [=================>............] - ETA: 11s - loss: 3.3329 - accuracy: 0.1402"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# training the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test, y_test), callbacks\u001b[39m=\u001b[39;49m[mcp_save], batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\caios\\Documents\\DataScience\\ComputerVision\\projects\\ActionRecognition\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\caios\\Documents\\DataScience\\ComputerVision\\projects\\ActionRecognition\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\caios\\Documents\\DataScience\\ComputerVision\\projects\\ActionRecognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\caios\\Documents\\DataScience\\ComputerVision\\projects\\ActionRecognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\caios\\Documents\\DataScience\\ComputerVision\\projects\\ActionRecognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\caios\\Documents\\DataScience\\ComputerVision\\projects\\ActionRecognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\caios\\Documents\\DataScience\\ComputerVision\\projects\\ActionRecognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\caios\\Documents\\DataScience\\ComputerVision\\projects\\ActionRecognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Users\\caios\\Documents\\DataScience\\ComputerVision\\projects\\ActionRecognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\caios\\Documents\\DataScience\\ComputerVision\\projects\\ActionRecognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "model.fit(X_train, y_train, epochs=200, validation_data=(X_test, y_test), callbacks=[mcp_save], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
