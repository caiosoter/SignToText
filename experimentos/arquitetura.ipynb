{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "\n",
    "# Pretrained models:\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
    "\n",
    "# Preprocessing Libraries:\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPool2D, GlobalMaxPool2D\n",
    "from keras.layers import LSTM, Input\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o Modelo pré-treinado Inception V3\n",
    "base_inception = InceptionV3(weights=\"imagenet\", include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo aceita imagens com resolução (299, 299)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing the initial layers of the pre-trained model:\n",
    "for layer in base_inception.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 100\n",
    "lstm_cells = 526\n",
    "sequencia_imagens = 10\n",
    "\n",
    "lstm_Inputs =  Input(shape=(sequencia_imagens, 2048))\n",
    "lstm_layer = LSTM(lstm_cells)(lstm_Inputs)\n",
    "\n",
    "# Implementando a camada Densa para classificação:\n",
    "dense_layer = Dense(1024, activation='relu')(lstm_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "# Criando o modelo CNN-LSTM:\n",
    "model = Model(inputs=lstm_Inputs, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilando o modelo:\n",
    "model.compile(optizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagens:\n",
    "batch_size = 32\n",
    "\n",
    "sequencia_de_imagens = np.random.rand(batch_size, \n",
    "                                      sequencia_imagens, 299, 299, 3)\n",
    "\n",
    "labels = np.random.randint(batch_size, num_classes)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# Preprocessa as imagens com base nos Steps do Inception V3:\n",
    "sequencia_de_imagens_preprocessed = preprocess_input(sequencia_de_imagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(sequencia_de_imagens_preprocessed, \n",
    "          y=labels, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
